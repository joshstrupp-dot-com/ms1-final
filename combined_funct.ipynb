{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "Below I have three --CODE BLOCKS--:\n",
    "1. SCRAPE: pulls information about a number of items and stores it in a json file. \n",
    "2. FASHION-CROP: looks at an image, uses object recognition to find fashion components of the image, extracts information about the components, and extracts new images that are cropped components of the original image. \n",
    "3. WORD-CLASSIFIER: uses a model to classify key words into categories like LOC (location), PER (person), ORG (organization), etc. \n",
    "\n",
    "------------------\n",
    "---------SCRAPE:---------\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Base URL components\n",
    "BASE_URL = \"https://www.si.edu/search/collection-images?edan_q=&edan_fq%5B0%5D=topic%3A%22Costume%22\"\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)\",\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "# Function to check robots.txt\n",
    "def can_scrape(url, user_agent='*'):\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    try:\n",
    "        response = requests.get(robots_url)\n",
    "        if response.status_code == 200:\n",
    "            from urllib.robotparser import RobotFileParser\n",
    "            rp = RobotFileParser()\n",
    "            rp.parse(response.text.splitlines())\n",
    "            return rp.can_fetch(user_agent, url)\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    return False  # If unable to fetch robots.txt, proceed with caution\n",
    "\n",
    "# Function to construct URL with page number\n",
    "def construct_url(page_number):\n",
    "    if page_number > 1:\n",
    "        # Insert page parameter after collection-images? for subsequent pages\n",
    "        page_param = f\"page={page_number - 1}&\"\n",
    "        url_parts = BASE_URL.split(\"?\")\n",
    "        return f\"{url_parts[0]}?{page_param}{url_parts[1]}\"\n",
    "    return BASE_URL\n",
    "\n",
    "# Function to extract item links from a page\n",
    "def get_item_links(soup):\n",
    "    item_links = []\n",
    "    for li in soup.find_all(\"li\", attrs={\"ogmt-id\": True}):\n",
    "        a_tag = li.find(\"a\", class_=\"inner\")\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            item_links.append(a_tag['href'])\n",
    "    return item_links\n",
    "\n",
    "# Function to extract information from an item's component_information page\n",
    "def extract_item_info(session, item_url):\n",
    "    full_url = f\"https://www.si.edu{item_url}\"\n",
    "    try:\n",
    "        response = session.get(full_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.warning(f\"Request exception for {full_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    content = response.content\n",
    "    logging.info(f\"Retrieved {full_url}\")\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Extract the h1 tag\n",
    "    h1_tag = soup.find(\"h1\")\n",
    "    if not h1_tag:\n",
    "        logging.warning(f\"No h1 tag found in {full_url}\")\n",
    "        return None\n",
    "    h1_text = h1_tag.get_text(strip=True)\n",
    "\n",
    "    # Initialize the nested dictionary\n",
    "    item_dict = {h1_text: {}}\n",
    "\n",
    "    # Extract the \"Museum\" information from the next <a> tag after <h1>\n",
    "    next_a_tag = h1_tag.find_next(\"a\")\n",
    "    if next_a_tag and next_a_tag.get_text(strip=True):\n",
    "        museum_name = next_a_tag.get_text(strip=True)\n",
    "        item_dict[h1_text][\"Museum\"] = museum_name\n",
    "    else:\n",
    "        logging.warning(f\"No Museum information found in {full_url}\")\n",
    "\n",
    "    # Iterate through all <dl> tags directly\n",
    "    for dl in soup.find_all(\"dl\"):\n",
    "        current_key = None\n",
    "        values = []\n",
    "        for child in dl.children:\n",
    "            if child.name == \"dt\":\n",
    "                if current_key and values:\n",
    "                    # Assign the collected values to the previous key\n",
    "                    item_dict[h1_text].setdefault(current_key, []).extend(values)\n",
    "                # Start a new key\n",
    "                current_key = child.get_text(strip=True)\n",
    "                values = []\n",
    "            elif child.name == \"dd\":\n",
    "                dd_text = child.get_text(strip=True)\n",
    "                values.append(dd_text)\n",
    "        # After the loop, assign the last collected values\n",
    "        if current_key and values:\n",
    "            item_dict[h1_text].setdefault(current_key, []).extend(values)\n",
    "\n",
    "    # Extracting the Image URL\n",
    "    media_inner_span = soup.find(\"span\", class_=\"media-inner\")\n",
    "    if media_inner_span:\n",
    "        a_tag = media_inner_span.find(\"a\", class_=\"modal-trigger image\")\n",
    "        if a_tag and a_tag.has_attr('data-source'):\n",
    "            image_url = a_tag['data-source']\n",
    "            item_dict[h1_text][\"Image_URL\"] = image_url\n",
    "            logging.info(f\"    Image URL extracted: {image_url}\")\n",
    "        else:\n",
    "            logging.warning(f\"    No image URL found in {full_url}\")\n",
    "    else:\n",
    "        logging.warning(f\"    No media-inner span found in {full_url}\")\n",
    "\n",
    "    return item_dict\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_smithsonian_collection(total_pages=3):\n",
    "    all_items = {}\n",
    "    total_scraped = 0\n",
    "\n",
    "    # Initialize a session\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": random.choice(USER_AGENTS)\n",
    "    })\n",
    "\n",
    "    # Check if scraping is allowed\n",
    "    # if not can_scrape(BASE_URL, session.headers[\"User-Agent\"]):\n",
    "    #     logging.warning(\"Scraping is not allowed by robots.txt. Exiting.\")\n",
    "    #     return all_items\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = construct_url(page)\n",
    "        logging.info(f\"Scraping Page {page}: {url}\")\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.warning(f\"Request exception for page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        item_links = get_item_links(soup)\n",
    "        logging.info(f\"Found {len(item_links)} items on Page {page}\")\n",
    "\n",
    "        for idx, item_link in enumerate(item_links, 1):\n",
    "            logging.info(f\"  Scraping Item {idx}: {item_link}\")\n",
    "            item_info = extract_item_info(session, item_link)\n",
    "            if item_info:\n",
    "                all_items.update(item_info)\n",
    "                total_scraped += 1\n",
    "            else:\n",
    "                logging.warning(f\"    Failed to extract info for {item_link}\")\n",
    "\n",
    "            # Add a random pause between 1 to 3 seconds\n",
    "            sleep_time = random.uniform(1, 3)\n",
    "            logging.info(f\"    Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        logging.info(f\"Completed scraping Page {page}\\n\")\n",
    "\n",
    "    logging.info(\"Scraping completed.\")\n",
    "    return all_items\n",
    "\n",
    "# Execute the scraping\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_data = scrape_smithsonian_collection(total_pages=3)\n",
    "    logging.info(f\"Scraping Completed. Total items scraped: {len(scraped_data)}\")\n",
    "    # Optionally, save the data to a file\n",
    "    with open('smithsonian_collection.json', 'w') as f:\n",
    "        json.dump(scraped_data, f, indent=4)\n",
    "\n",
    "-----------WORD-CLASSIFIER----------\n",
    "from transformers import pipeline\n",
    "\n",
    "with open('/Users/joshstrupp/Documents/Working/Educational/MSDV/ms1-final/notebook_explore/notebooks/smithsonian_collection.json', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Get results and filter by score\n",
    "results = token_classifier(text)\n",
    "filtered_results = [entity for entity in results if entity['score'] >= 0.8]\n",
    "\n",
    "# Display filtered results\n",
    "filtered_results\n",
    "\n",
    "\n",
    "-----------------FASHION-CROP-------------\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"detection-datasets/fashionpedia\")\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoImageProcessor.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'Chippewa.png' with the actual path to your image\n",
    "image_path = 'test2.png'\n",
    "\n",
    "# Load the image and ensure it's in RGB format\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Debugging: Print the shape of the image array\n",
    "print('Image shape:', image_array.shape)  # Should be (height, width, 3)\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(images=image_array, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Post-process the outputs to get object detection results\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs, threshold=0.5, target_sizes=[image.size[::-1]]\n",
    ")\n",
    "result = results[0]  # Since we have only one image\n",
    "\n",
    "import json\n",
    "\n",
    "# Load category mappings\n",
    "with open('/Users/joshstrupp/Documents/Working/Educational/MSDV/ms1-final/fashionpedia-api/data/demo/category_attributes_descriptions.json', 'r') as f:\n",
    "    category_data = json.load(f)\n",
    "\n",
    "# Create mappings from category IDs to names and supercategories\n",
    "categories = category_data['categories']\n",
    "category_id_to_name = {category['id']: category['name'] for category in categories}\n",
    "category_id_to_supercategory = {category['id']: category['supercategory'] for category in categories}\n",
    "\n",
    "# Create a mapping from label IDs to category names (from the model)\n",
    "label_mappings = model.config.id2label  # This maps label IDs to category names\n",
    "\n",
    "# Create a mapping from category names to supercategories\n",
    "name_to_supercategory = {category['name']: category['supercategory'] for category in categories}\n",
    "# Prepare the objects data\n",
    "objects = {\n",
    "    'bbox_id': [],\n",
    "    'category': [],\n",
    "    'bbox': [],\n",
    "    'area': [],\n",
    "    'supercategory': [],\n",
    "    'name': []\n",
    "}\n",
    "\n",
    "for idx in range(len(result['scores'])):\n",
    "    score = result['scores'][idx].item()\n",
    "    label_id = result['labels'][idx].item()\n",
    "    box = result['boxes'][idx].tolist()  # [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    # Compute area\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "    # Get category name from label ID\n",
    "    name = label_mappings.get(label_id, 'Unknown')\n",
    "    supercategory = name_to_supercategory.get(name, 'Unknown')\n",
    "\n",
    "    # Map category name back to category ID from the dataset if needed\n",
    "    category_id = next((id for id, n in category_id_to_name.items() if n == name), label_id)\n",
    "\n",
    "    # Append to objects\n",
    "    objects['bbox_id'].append(idx)\n",
    "    objects['category'].append(category_id)\n",
    "    objects['bbox'].append(box)\n",
    "    objects['area'].append(area)\n",
    "    objects['name'].append(name)\n",
    "    objects['supercategory'].append(supercategory)\n",
    "\n",
    "# Get image dimensions\n",
    "width, height = image.size\n",
    "\n",
    "# Compile the final output\n",
    "output = {\n",
    "    'image_id': 0,  # Assign an ID to your image\n",
    "    'image': image,\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'objects': objects\n",
    "}\n",
    "\n",
    "# Print the analysis\n",
    "print(\"Image ID:\", output['image_id'])\n",
    "print(\"Width:\", output['width'])\n",
    "print(\"Height:\", output['height'])\n",
    "print(\"Objects Detected:\")\n",
    "for i in range(len(objects['bbox_id'])):\n",
    "    print(f\"  Object {i+1}:\")\n",
    "    print(f\"    Bounding Box ID: {objects['bbox_id'][i]}\")\n",
    "    print(f\"    Category ID: {objects['category'][i]}\")\n",
    "    print(f\"    Name: {objects['name'][i]}\")\n",
    "    print(f\"    Supercategory: {objects['supercategory'][i]}\")\n",
    "    print(f\"    Bounding Box: {objects['bbox'][i]}\")\n",
    "    print(f\"    Area: {objects['area'][i]}\")\n",
    "\n",
    "def analyze_image(image_path, category_json_path='category_attributes_descriptions.json'):\n",
    "    \"\"\"\n",
    "    Analyzes an image using the Fashionpedia model and returns a DataFrame with the outputs.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): The path to the image file.\n",
    "    - category_json_path (str): The path to the category attributes JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import json\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Load your image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Ensure image is in RGB format\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # 2. Load the processor and model\n",
    "    processor = AutoImageProcessor.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "    model = AutoModelForObjectDetection.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "\n",
    "    # 3. Prepare the image\n",
    "    inputs = processor(images=image_array, return_tensors=\"pt\")\n",
    "\n",
    "    # 4. Run inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # 5. Process the outputs\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, threshold=0.8, target_sizes=[image.size[::-1]]\n",
    "    )\n",
    "    result = results[0]\n",
    "\n",
    "    # 6. Map category IDs to names and supercategories\n",
    "    with open(category_json_path, 'r') as f:\n",
    "        category_data = json.load(f)\n",
    "    categories = category_data['categories']\n",
    "    category_id_to_name = {category['id']: category['name'] for category in categories}\n",
    "    category_id_to_supercategory = {category['id']: category['supercategory'] for category in categories}\n",
    "\n",
    "    label_mappings = model.config.id2label\n",
    "    name_to_supercategory = {category['name']: category['supercategory'] for category in categories}\n",
    "\n",
    "    # 7. Prepare data for DataFrame\n",
    "    data = []\n",
    "    for idx in range(len(result['scores'])):\n",
    "        score = result['scores'][idx].item()\n",
    "        label_id = result['labels'][idx].item()\n",
    "        box = result['boxes'][idx].tolist()\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "        name = label_mappings.get(label_id, 'Unknown')\n",
    "        supercategory = name_to_supercategory.get(name, 'Unknown')\n",
    "        category_id = next((id for id, n in category_id_to_name.items() if n == name), label_id)\n",
    "\n",
    "        data.append({\n",
    "            'bbox_id': idx,\n",
    "            'category_id': category_id,\n",
    "            'name': name,\n",
    "            'supercategory': supercategory,\n",
    "            'bbox': box,\n",
    "            'area': area,\n",
    "            'score': score\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Replace 'path/to/your/image.jpg' with the actual path to your image\n",
    "image_path = 'test2.png'\n",
    "\n",
    "# Optionally, specify the path to your category attributes JSON file\n",
    "category_json_path = '../category_attributes_descriptions.json'\n",
    "\n",
    "# Call the function\n",
    "df = analyze_image(image_path, category_json_path)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def save_cropped_images(image_path, df, output_dir='cropped_images'):\n",
    "    \"\"\"\n",
    "    Crops the original image according to the bounding boxes and saves the cropped images.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the original image.\n",
    "    - df (pandas.DataFrame): DataFrame containing the detection results.\n",
    "    - output_dir (str): Directory where the cropped images will be saved.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import os\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Ensure image is in RGB format\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get image dimensions\n",
    "    width, height = image.size\n",
    "\n",
    "    # Loop over each detection\n",
    "    for idx, row in df.iterrows():\n",
    "        bbox = row['bbox']\n",
    "        name = row['name']\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "\n",
    "        # Clip coordinates to image bounds\n",
    "        x_min = max(0, min(width, x_min))\n",
    "        y_min = max(0, min(height, y_min))\n",
    "        x_max = max(0, min(width, x_max))\n",
    "        y_max = max(0, min(height, y_max))\n",
    "\n",
    "        # Check for valid crop\n",
    "        if x_max > x_min and y_max > y_min:\n",
    "            # Crop the image\n",
    "            cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "            # Ensure cropped image is in RGB mode\n",
    "            if cropped_image.mode != 'RGB':\n",
    "                cropped_image = cropped_image.convert('RGB')\n",
    "\n",
    "            # Create a unique filename\n",
    "            image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            filename = f\"{image_name}_{name.replace(' ', '_')}_{idx}.jpg\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            try:\n",
    "                # Save the cropped image\n",
    "                cropped_image.save(output_path, format='JPEG')\n",
    "                print(f\"Saved cropped image: {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save {output_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping invalid crop for detection {idx}\")\n",
    "\n",
    "# Assume df is the DataFrame obtained from analyze_image\n",
    "image_path = 'Angola Costume.png'  # Replace with your image path\n",
    "\n",
    "# Check if output directory exists, if not create it\n",
    "if not os.path.exists('cropped_images'):\n",
    "    os.makedirs('cropped_images')\n",
    "\n",
    "# Call the function to save cropped images, ensuring unique filenames\n",
    "existing_files = set(os.listdir('cropped_images'))\n",
    "counter = 1\n",
    "while any(f\"{os.path.splitext(os.path.basename(image_path))[0]}_{counter}\" in f for f in existing_files):\n",
    "    counter += 1\n",
    "save_cropped_images(image_path, df, output_dir='cropped_images')\n",
    "\n",
    "---------------------\n",
    "\n",
    "The objective is to combine this into one python script that\n",
    "\n",
    "1. Completes the SCRAPE and saves all data to a dataframe where h1_text is in first column \"title\", and remaining columns are made up of dt values, while dd (and dd lists) are cell contents. E.g.:\n",
    "\n",
    "Columns: Title, Museum, Names, Collection Photographer, ...\n",
    "Vals: Lillian Evanti wears costume from Lucia di Lammermour, Anacostia Community Museum, [\"Evanti, Lillian, Mme. (Lillian Evans Tibbs), 1890-1967\"], [\"Apeda Studio (New York, N.Y.)\", \"Camuzzi, M.\", \"Harris & Ewing\"]... \n",
    "\n",
    "2. Use the WORD-CLASSIFIER to look at each string in the df and extract okenized words that have filtered_results, i.e. have an assigned PER, ORG, LOC, ets. (Currently using local json: with open('/Users/joshstrupp/Documents/Working/Educational/MSDV/ms1-final/notebook_explore/notebooks/smithsonian_collection.json', 'r') as file:\n",
    "    text = file.read() — want to replace the json with the df information from step 1) Place in a new column as a comma separated list that is named \"<dt>.[entity_group]>\". E.g. for the below, there would be a new column that is produced called \"Collection Photographer.LOC\" and it might include \"Apeda Studio, New York\". Then \"Collection Photographer.PER\" that includes [Camuzzi,M., Harris & Ewing], etc: \n",
    "\"Lillian Evanti wears costume from Lucia di Lammermour\": {\n",
    "        ...\n",
    "        \"Collection Photographer\": [\n",
    "            \"Apeda Studio (New York, N.Y.)\",\n",
    "            \"Camuzzi, M.\",\n",
    "            \"Harris & Ewing\"\n",
    "        ],\n",
    "\n",
    "3. As we scrape and retrieve the image_url, pass the image from image_url through the FASHION-CROP. Here, we'll a new column called \"cropped_objects\" that contains a dictionary of dictionaries using Name, Supercategory, Bounding Box, and Area. E.g. {Object 1: {Name: neckline}, {Supercategory: garment parts}...},{Object 2: {Name: something}, {Supercategory: something else}...}\n",
    "\n",
    "4. Also in FASHION-CROP we will need to extract the cropped_images. These will be saved to a folder using the current image path structure, then that path will be added to the dataframe's \"cropped_objects\" dict at the end for each object identified, e.g. {...{cropped_image_path:/whatever/the/path/to/download/is.jpg}}\n",
    "\n",
    "\n",
    "I imagine this one will take you a while. Take your time. Place \"prints\" to show progress as the function runs. And thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Scraping Page 1: https://www.si.edu/search/collection-images?edan_q=&edan_fq%5B0%5D=topic%3A%22Costume%22\n",
      "Found 54 items on Page 1\n",
      "  Scraping Item 1: /object/archives/components/sova-acma-06-016-ref18\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-acma-06-016-ref18\n",
      "/var/folders/pc/j_zmpc9s0g9frv2jzfv1l51c0000gn/T/ipykernel_16324/79763558.py:100: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  screen_image_link = soup.find('a', text='Screen Image')\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3AACM-acma_PH2003_7063_007_01edit.jpg\n",
      "    Sleeping for 2.25 seconds...\n",
      "  Scraping Item 2: /object/archives/components/sova-eepa-1985-014-ref1532\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1532\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-20-85.jpg\n",
      "    Sleeping for 1.15 seconds...\n",
      "  Scraping Item 3: /object/archives/components/sova-eepa-1985-014-ref1254\n",
      "Request exception for https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1254: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "    Failed to extract info for /object/archives/components/sova-eepa-1985-014-ref1254\n",
      "    Sleeping for 2.29 seconds...\n",
      "  Scraping Item 4: /object/archives/components/sova-eepa-1985-014-ref1873\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1873\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-15-05.jpg\n",
      "    Sleeping for 2.49 seconds...\n",
      "  Scraping Item 5: /object/archives/components/sova-eepa-1985-014-ref1285\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1285\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-20-13.jpg\n",
      "    Sleeping for 2.30 seconds...\n",
      "  Scraping Item 6: /object/archives/components/sova-eepa-1985-014-ref1257\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1257\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AE-24-10.jpg\n",
      "    Sleeping for 1.44 seconds...\n",
      "  Scraping Item 7: /object/archives/components/sova-eepa-1985-014-ref1883\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1883\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-20-92-000002.jpg\n",
      "    Sleeping for 1.72 seconds...\n",
      "  Scraping Item 8: /object/archives/components/sova-eepa-1985-014-ref4763\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref4763\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-IV-17-21.jpg\n",
      "    Sleeping for 1.20 seconds...\n",
      "  Scraping Item 9: /object/archives/components/sova-eepa-1985-014-ref3975\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref3975\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-DM-18-01-000002.jpg\n",
      "    Sleeping for 2.43 seconds...\n",
      "  Scraping Item 10: /object/archives/components/sova-eepa-1985-014-ref4977\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref4977\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-IV-08-10.jpg\n",
      "    Sleeping for 2.90 seconds...\n",
      "  Scraping Item 11: /object/archives/components/sova-eepa-1985-014-ref6312\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref6312\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-MR-20-42.jpg\n",
      "    Sleeping for 1.81 seconds...\n",
      "  Scraping Item 12: /object/archives/components/sova-eepa-1985-014-ref4500\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref4500\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-FT-20-05.jpg\n",
      "    Sleeping for 2.52 seconds...\n",
      "  Scraping Item 13: /object/archives/components/sova-eepa-1985-014-ref5417\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5417\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-KE-05-01.jpg\n",
      "    Sleeping for 2.06 seconds...\n",
      "  Scraping Item 14: /object/archives/components/sova-eepa-1985-014-ref3522\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref3522\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-CG-23-06-000002.jpg\n",
      "    Sleeping for 2.65 seconds...\n",
      "  Scraping Item 15: /object/archives/components/sova-eepa-1985-014-ref5523\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5523\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-11.jpg\n",
      "    Sleeping for 1.59 seconds...\n",
      "  Scraping Item 16: /object/archives/components/sova-eepa-1985-014-ref871\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref871\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140031-03.jpg\n",
      "    Sleeping for 2.12 seconds...\n",
      "  Scraping Item 17: /object/archives/components/sova-eepa-1985-014-ref944\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref944\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140085-01.jpg\n",
      "    Sleeping for 2.50 seconds...\n",
      "  Scraping Item 18: /object/archives/components/sova-eepa-1985-014-ref5573\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5573\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-12.jpg\n",
      "    Sleeping for 1.81 seconds...\n",
      "  Scraping Item 19: /object/archives/components/sova-eepa-1985-014-ref5543\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5543\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-35.jpg\n",
      "    Sleeping for 1.20 seconds...\n",
      "  Scraping Item 20: /object/archives/components/sova-eepa-1985-014-ref5538\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5538\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-30.jpg\n",
      "    Sleeping for 2.67 seconds...\n",
      "  Scraping Item 21: /object/archives/components/sova-eepa-1985-014-ref5527\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5527\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-15.jpg\n",
      "    Sleeping for 2.57 seconds...\n",
      "  Scraping Item 22: /object/archives/components/sova-eepa-1985-014-ref5716\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5716\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-02.jpg\n",
      "    Sleeping for 1.94 seconds...\n",
      "  Scraping Item 23: /object/archives/components/sova-eepa-1985-014-ref957\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref957\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140093-01.jpg\n",
      "    Sleeping for 1.29 seconds...\n",
      "  Scraping Item 24: /object/archives/components/sova-eepa-1985-014-ref5542\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5542\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-34.jpg\n",
      "    Sleeping for 1.15 seconds...\n",
      "  Scraping Item 25: /object/archives/components/sova-eepa-1985-014-ref971\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref971\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140104.jpg\n",
      "    Sleeping for 1.16 seconds...\n",
      "  Scraping Item 26: /object/archives/components/sova-eepa-1985-014-ref947\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref947\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140086-02.jpg\n",
      "    Sleeping for 1.42 seconds...\n",
      "  Scraping Item 27: /object/archives/components/sova-eepa-1985-014-ref940\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref940\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140082-01.jpg\n",
      "    Sleeping for 1.88 seconds...\n",
      "  Scraping Item 28: /object/archives/components/sova-eepa-1985-014-ref991\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref991\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140116.jpg\n",
      "    Sleeping for 2.36 seconds...\n",
      "  Scraping Item 29: /object/archives/components/sova-eepa-1985-014-ref5547\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5547\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-39.jpg\n",
      "    Sleeping for 2.89 seconds...\n",
      "  Scraping Item 30: /object/archives/components/sova-eepa-1985-014-ref5515\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5515\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-08.jpg\n",
      "    Sleeping for 1.84 seconds...\n",
      "  Scraping Item 31: /object/archives/components/sova-eepa-1985-014-ref1003\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1003\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140127.jpg\n",
      "    Sleeping for 1.78 seconds...\n",
      "  Scraping Item 32: /object/archives/components/sova-eepa-1985-014-ref5556\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5556\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-45.jpg\n",
      "    Sleeping for 2.92 seconds...\n",
      "  Scraping Item 33: /object/archives/components/sova-eepa-1985-014-ref5554\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5554\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-51.jpg\n",
      "    Sleeping for 1.46 seconds...\n",
      "  Scraping Item 34: /object/archives/components/sova-eepa-1985-014-ref974\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref974\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140106-01.jpg\n",
      "    Sleeping for 1.88 seconds...\n",
      "  Scraping Item 35: /object/archives/components/sova-eepa-1985-014-ref911\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref911\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140062-01.jpg\n",
      "    Sleeping for 1.31 seconds...\n",
      "  Scraping Item 36: /object/archives/components/sova-eepa-1985-014-ref973\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref973\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140105-02.jpg\n",
      "    Sleeping for 2.65 seconds...\n",
      "  Scraping Item 37: /object/archives/components/sova-eepa-1985-014-ref5563\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5563\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-02.jpg\n",
      "    Sleeping for 1.34 seconds...\n",
      "  Scraping Item 38: /object/archives/components/sova-eepa-1985-014-ref5577\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5577\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-18.jpg\n",
      "    Sleeping for 1.91 seconds...\n",
      "  Scraping Item 39: /object/archives/components/sova-eepa-1985-014-ref972\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref972\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140105-01.jpg\n",
      "    Sleeping for 1.72 seconds...\n",
      "  Scraping Item 40: /object/archives/components/sova-eepa-1985-014-ref5828\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5828\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-38-02.jpg\n",
      "    Sleeping for 1.75 seconds...\n",
      "  Scraping Item 41: /object/archives/components/sova-eepa-1985-014-ref5692\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5692\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-16-04.jpg\n",
      "    Sleeping for 1.87 seconds...\n",
      "  Scraping Item 42: /object/archives/components/sova-eepa-1985-014-ref5516\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5516\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-07.jpg\n",
      "    Sleeping for 1.25 seconds...\n",
      "  Scraping Item 43: /object/archives/components/sova-eepa-1985-014-ref1001\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1001\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140125.jpg\n",
      "    Sleeping for 2.84 seconds...\n",
      "  Scraping Item 44: /object/archives/components/sova-eepa-1985-014-ref5576\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5576\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-16.jpg\n",
      "    Sleeping for 1.42 seconds...\n",
      "  Scraping Item 45: /object/archives/components/sova-eepa-1985-014-ref5534\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5534\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-26.jpg\n",
      "    Sleeping for 1.97 seconds...\n",
      "  Scraping Item 46: /object/archives/components/sova-eepa-1985-014-ref5546\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5546\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-38.jpg\n",
      "    Sleeping for 2.64 seconds...\n",
      "  Scraping Item 47: /object/archives/components/sova-eepa-1985-014-ref5555\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5555\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-52.jpg\n",
      "    Sleeping for 1.89 seconds...\n",
      "  Scraping Item 48: /object/archives/components/sova-eepa-1985-014-ref1000\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref1000\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140124.jpg\n",
      "    Sleeping for 1.83 seconds...\n",
      "  Scraping Item 49: /object/archives/components/sova-eepa-1985-014-ref980\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref980\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140109.jpg\n",
      "    Sleeping for 1.38 seconds...\n",
      "  Scraping Item 50: /object/archives/components/sova-eepa-1985-014-ref5648\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5648\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-08-03.jpg\n",
      "    Sleeping for 2.03 seconds...\n",
      "  Scraping Item 51: /object/archives/components/sova-eepa-1985-014-ref870\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref870\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140031-02.jpg\n",
      "    Sleeping for 2.94 seconds...\n",
      "  Scraping Item 52: /object/archives/components/sova-eepa-1985-014-ref5829\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5829\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-38-03.jpg\n",
      "    Sleeping for 1.25 seconds...\n",
      "  Scraping Item 53: /object/archives/components/sova-eepa-1985-014-ref5551\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5551\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-43.jpg\n",
      "    Sleeping for 2.24 seconds...\n",
      "  Scraping Item 54: /object/archives/components/sova-eepa-1985-014-ref5691\n",
      "Retrieved https://www.si.edu/object/archives/components/sova-eepa-1985-014-ref5691\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-16-03.jpg\n",
      "    Sleeping for 1.47 seconds...\n",
      "Completed scraping Page 1\n",
      "\n",
      "Scraping completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Completed. Total items scraped: 53\n",
      "Extracting entities...\n",
      "Processing images...\n",
      "Processing image for row 0: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3AACM-acma_PH2003_7063_007_01edit.jpg\n",
      "Saved cropped image: cropped_images/image_0_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_0_dress_1.jpg\n",
      "Saved cropped image: cropped_images/image_0_sleeve_2.jpg\n",
      "Saved cropped image: cropped_images/image_0_neckline_3.jpg\n",
      "Failed to process image cropped_images/image_0.jpg: Incompatible indexer with Series\n",
      "Processing image for row 1: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-20-85.jpg\n",
      "Saved cropped image: cropped_images/image_1_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_1_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_1_dress_2.jpg\n",
      "Processing image for row 2: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-15-05.jpg\n",
      "Saved cropped image: cropped_images/image_2_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_2_sleeve_1.jpg\n",
      "Processing image for row 3: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-20-13.jpg\n",
      "Saved cropped image: cropped_images/image_3_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_3_neckline_1.jpg\n",
      "Saved cropped image: cropped_images/image_3_top,_t-shirt,_sweatshirt_2.jpg\n",
      "Saved cropped image: cropped_images/image_3_sleeve_3.jpg\n",
      "Processing image for row 4: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AE-24-10.jpg\n",
      "Processing image for row 5: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-AO-20-92-000002.jpg\n",
      "Saved cropped image: cropped_images/image_5_collar_0.jpg\n",
      "Saved cropped image: cropped_images/image_5_lapel_1.jpg\n",
      "Saved cropped image: cropped_images/image_5_jacket_2.jpg\n",
      "Saved cropped image: cropped_images/image_5_sleeve_3.jpg\n",
      "Saved cropped image: cropped_images/image_5_sleeve_4.jpg\n",
      "Processing image for row 6: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-IV-17-21.jpg\n",
      "Saved cropped image: cropped_images/image_6_pants_0.jpg\n",
      "Saved cropped image: cropped_images/image_6_pants_1.jpg\n",
      "Saved cropped image: cropped_images/image_6_pants_2.jpg\n",
      "Saved cropped image: cropped_images/image_6_shoe_3.jpg\n",
      "Saved cropped image: cropped_images/image_6_pants_4.jpg\n",
      "Saved cropped image: cropped_images/image_6_pants_5.jpg\n",
      "Saved cropped image: cropped_images/image_6_shoe_6.jpg\n",
      "Saved cropped image: cropped_images/image_6_pants_7.jpg\n",
      "Saved cropped image: cropped_images/image_6_pants_8.jpg\n",
      "Saved cropped image: cropped_images/image_6_bag,_wallet_9.jpg\n",
      "Processing image for row 7: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-DM-18-01-000002.jpg\n",
      "Processing image for row 8: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-IV-08-10.jpg\n",
      "Saved cropped image: cropped_images/image_8_shoe_0.jpg\n",
      "Saved cropped image: cropped_images/image_8_tights,_stockings_1.jpg\n",
      "Saved cropped image: cropped_images/image_8_sleeve_2.jpg\n",
      "Saved cropped image: cropped_images/image_8_shoe_3.jpg\n",
      "Saved cropped image: cropped_images/image_8_shoe_4.jpg\n",
      "Saved cropped image: cropped_images/image_8_tights,_stockings_5.jpg\n",
      "Saved cropped image: cropped_images/image_8_shoe_6.jpg\n",
      "Saved cropped image: cropped_images/image_8_sleeve_7.jpg\n",
      "Processing image for row 9: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-MR-20-42.jpg\n",
      "Saved cropped image: cropped_images/image_9_sleeve_0.jpg\n",
      "Processing image for row 10: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-FT-20-05.jpg\n",
      "Saved cropped image: cropped_images/image_10_pocket_0.jpg\n",
      "Saved cropped image: cropped_images/image_10_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_10_sleeve_2.jpg\n",
      "Processing image for row 11: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-KE-05-01.jpg\n",
      "Processing image for row 12: https://ids.si.edu/ids/deliveryService?max_w=800&id=damsmdm%3ANMAfA-CG-23-06-000002.jpg\n",
      "Processing image for row 13: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-11.jpg\n",
      "Processing image for row 14: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140031-03.jpg\n",
      "Saved cropped image: cropped_images/image_14_sleeve_0.jpg\n",
      "Processing image for row 15: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140085-01.jpg\n",
      "Saved cropped image: cropped_images/image_15_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_15_sleeve_1.jpg\n",
      "Processing image for row 16: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-12.jpg\n",
      "Saved cropped image: cropped_images/image_16_sleeve_0.jpg\n",
      "Processing image for row 17: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-35.jpg\n",
      "Processing image for row 18: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-30.jpg\n",
      "Processing image for row 19: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-15.jpg\n",
      "Saved cropped image: cropped_images/image_19_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_19_hat_1.jpg\n",
      "Saved cropped image: cropped_images/image_19_shoe_2.jpg\n",
      "Processing image for row 20: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-02.jpg\n",
      "Saved cropped image: cropped_images/image_20_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_20_top,_t-shirt,_sweatshirt_1.jpg\n",
      "Saved cropped image: cropped_images/image_20_sleeve_2.jpg\n",
      "Saved cropped image: cropped_images/image_20_neckline_3.jpg\n",
      "Processing image for row 21: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140093-01.jpg\n",
      "Saved cropped image: cropped_images/image_21_dress_0.jpg\n",
      "Saved cropped image: cropped_images/image_21_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_21_sleeve_2.jpg\n",
      "Processing image for row 22: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-34.jpg\n",
      "Saved cropped image: cropped_images/image_22_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_22_dress_1.jpg\n",
      "Saved cropped image: cropped_images/image_22_sleeve_2.jpg\n",
      "Processing image for row 23: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140104.jpg\n",
      "Saved cropped image: cropped_images/image_23_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_23_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_23_pants_2.jpg\n",
      "Saved cropped image: cropped_images/image_23_belt_3.jpg\n",
      "Saved cropped image: cropped_images/image_23_shoe_4.jpg\n",
      "Saved cropped image: cropped_images/image_23_shoe_5.jpg\n",
      "Saved cropped image: cropped_images/image_23_pants_6.jpg\n",
      "Saved cropped image: cropped_images/image_23_sleeve_7.jpg\n",
      "Processing image for row 24: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140086-02.jpg\n",
      "Processing image for row 25: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140082-01.jpg\n",
      "Saved cropped image: cropped_images/image_25_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_25_sleeve_1.jpg\n",
      "Processing image for row 26: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140116.jpg\n",
      "Saved cropped image: cropped_images/image_26_sleeve_0.jpg\n",
      "Processing image for row 27: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-39.jpg\n",
      "Saved cropped image: cropped_images/image_27_dress_0.jpg\n",
      "Processing image for row 28: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-08.jpg\n",
      "Saved cropped image: cropped_images/image_28_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_28_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_28_sleeve_2.jpg\n",
      "Processing image for row 29: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140127.jpg\n",
      "Processing image for row 30: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-45.jpg\n",
      "Saved cropped image: cropped_images/image_30_neckline_0.jpg\n",
      "Processing image for row 31: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-51.jpg\n",
      "Saved cropped image: cropped_images/image_31_sleeve_0.jpg\n",
      "Processing image for row 32: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140106-01.jpg\n",
      "Saved cropped image: cropped_images/image_32_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_32_belt_1.jpg\n",
      "Saved cropped image: cropped_images/image_32_belt_2.jpg\n",
      "Saved cropped image: cropped_images/image_32_pants_3.jpg\n",
      "Saved cropped image: cropped_images/image_32_pants_4.jpg\n",
      "Saved cropped image: cropped_images/image_32_sleeve_5.jpg\n",
      "Saved cropped image: cropped_images/image_32_pants_6.jpg\n",
      "Saved cropped image: cropped_images/image_32_pants_7.jpg\n",
      "Saved cropped image: cropped_images/image_32_sleeve_8.jpg\n",
      "Saved cropped image: cropped_images/image_32_sleeve_9.jpg\n",
      "Processing image for row 33: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140062-01.jpg\n",
      "Saved cropped image: cropped_images/image_33_sleeve_0.jpg\n",
      "Processing image for row 34: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140105-02.jpg\n",
      "Saved cropped image: cropped_images/image_34_neckline_0.jpg\n",
      "Processing image for row 35: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-02.jpg\n",
      "Saved cropped image: cropped_images/image_35_sleeve_0.jpg\n",
      "Processing image for row 36: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-18.jpg\n",
      "Saved cropped image: cropped_images/image_36_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_36_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_36_sleeve_2.jpg\n",
      "Processing image for row 37: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140105-01.jpg\n",
      "Processing image for row 38: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-38-02.jpg\n",
      "Saved cropped image: cropped_images/image_38_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_38_shoe_1.jpg\n",
      "Saved cropped image: cropped_images/image_38_shoe_2.jpg\n",
      "Saved cropped image: cropped_images/image_38_sleeve_3.jpg\n",
      "Saved cropped image: cropped_images/image_38_sleeve_4.jpg\n",
      "Processing image for row 39: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-16-04.jpg\n",
      "Processing image for row 40: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-07.jpg\n",
      "Saved cropped image: cropped_images/image_40_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_40_sleeve_1.jpg\n",
      "Processing image for row 41: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140125.jpg\n",
      "Saved cropped image: cropped_images/image_41_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_41_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_41_headband,_head_covering,_hair_accessory_2.jpg\n",
      "Saved cropped image: cropped_images/image_41_dress_3.jpg\n",
      "Saved cropped image: cropped_images/image_41_sleeve_4.jpg\n",
      "Processing image for row 42: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-05-16.jpg\n",
      "Saved cropped image: cropped_images/image_42_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_42_top,_t-shirt,_sweatshirt_1.jpg\n",
      "Processing image for row 43: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-26.jpg\n",
      "Processing image for row 44: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-38.jpg\n",
      "Saved cropped image: cropped_images/image_44_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_44_neckline_1.jpg\n",
      "Saved cropped image: cropped_images/image_44_dress_2.jpg\n",
      "Saved cropped image: cropped_images/image_44_sleeve_3.jpg\n",
      "Processing image for row 45: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-52.jpg\n",
      "Processing image for row 46: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140124.jpg\n",
      "Saved cropped image: cropped_images/image_46_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_46_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_46_neckline_2.jpg\n",
      "Saved cropped image: cropped_images/image_46_dress_3.jpg\n",
      "Saved cropped image: cropped_images/image_46_sleeve_4.jpg\n",
      "Processing image for row 47: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140109.jpg\n",
      "Saved cropped image: cropped_images/image_47_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_47_sleeve_1.jpg\n",
      "Saved cropped image: cropped_images/image_47_neckline_2.jpg\n",
      "Saved cropped image: cropped_images/image_47_headband,_head_covering,_hair_accessory_3.jpg\n",
      "Saved cropped image: cropped_images/image_47_dress_4.jpg\n",
      "Saved cropped image: cropped_images/image_47_sleeve_5.jpg\n",
      "Processing image for row 48: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-08-03.jpg\n",
      "Processing image for row 49: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-EEPA_1985-140031-02.jpg\n",
      "Saved cropped image: cropped_images/image_49_sleeve_0.jpg\n",
      "Saved cropped image: cropped_images/image_49_sleeve_1.jpg\n",
      "Processing image for row 50: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-38-03.jpg\n",
      "Processing image for row 51: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-20-43.jpg\n",
      "Saved cropped image: cropped_images/image_51_neckline_0.jpg\n",
      "Saved cropped image: cropped_images/image_51_sleeve_1.jpg\n",
      "Processing image for row 52: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAfA-MG-16-03.jpg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Base URL components\n",
    "BASE_URL = \"https://www.si.edu/search/collection-images?edan_q=&edan_fq%5B0%5D=topic%3A%22Costume%22\"\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)\",\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "# Function to construct URL with page number\n",
    "def construct_url(page_number):\n",
    "    if page_number > 1:\n",
    "        # Insert page parameter after collection-images? for subsequent pages\n",
    "        page_param = f\"page={page_number - 1}&\"\n",
    "        url_parts = BASE_URL.split(\"?\")\n",
    "        return f\"{url_parts[0]}?{page_param}{url_parts[1]}\"\n",
    "    return BASE_URL\n",
    "\n",
    "# Function to extract item links from a page\n",
    "def get_item_links(soup):\n",
    "    item_links = []\n",
    "    for li in soup.find_all(\"li\", attrs={\"ogmt-id\": True}):\n",
    "        a_tag = li.find(\"a\", class_=\"inner\")\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            item_links.append(a_tag['href'])\n",
    "    return item_links\n",
    "\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "\n",
    "def extract_item_info(session, item_url):\n",
    "    full_url = f\"https://www.si.edu{item_url}\"\n",
    "    try:\n",
    "        response = session.get(full_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.warning(f\"Request exception for {full_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    content = response.content\n",
    "    logging.info(f\"Retrieved {full_url}\")\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Extract the h1 tag\n",
    "    h1_tag = soup.find(\"h1\")\n",
    "    if not h1_tag:\n",
    "        logging.warning(f\"No h1 tag found in {full_url}\")\n",
    "        return None\n",
    "    h1_text = h1_tag.get_text(strip=True)\n",
    "\n",
    "    # Initialize the item dictionary\n",
    "    item_dict = {'Title': h1_text}\n",
    "\n",
    "    # Extract the \"Museum\" information from the next <a> tag after <h1>\n",
    "    next_a_tag = h1_tag.find_next(\"a\")\n",
    "    if next_a_tag and next_a_tag.get_text(strip=True):\n",
    "        museum_name = next_a_tag.get_text(strip=True)\n",
    "        item_dict[\"Museum\"] = museum_name\n",
    "    else:\n",
    "        logging.warning(f\"No Museum information found in {full_url}\")\n",
    "\n",
    "    # Iterate through all <dl> tags directly\n",
    "    for dl in soup.find_all(\"dl\"):\n",
    "        current_key = None\n",
    "        values = []\n",
    "        for child in dl.children:\n",
    "            if child.name == \"dt\":\n",
    "                if current_key and values:\n",
    "                    # Assign the collected values to the previous key\n",
    "                    item_dict[current_key] = values\n",
    "                # Start a new key\n",
    "                current_key = child.get_text(strip=True)\n",
    "                values = []\n",
    "            elif child.name == \"dd\":\n",
    "                dd_text = child.get_text(strip=True)\n",
    "                values.append(dd_text)\n",
    "        # After the loop, assign the last collected values\n",
    "        if current_key and values:\n",
    "            item_dict[current_key] = values\n",
    "\n",
    "    # Try to extract the 'Screen Image' link\n",
    "    screen_image_link = soup.find('a', text='Screen Image')\n",
    "    if screen_image_link and screen_image_link.has_attr('href'):\n",
    "        image_url = screen_image_link['href']\n",
    "        if not image_url.startswith('http'):\n",
    "            image_url = f\"https://ids.si.edu{image_url}\"\n",
    "        item_dict[\"Image_URL\"] = image_url\n",
    "        logging.info(f\"    Screen Image URL extracted: {image_url}\")\n",
    "    else:\n",
    "        # Fallback method: extract from <img id='edan-image'> tag\n",
    "        img_tag = soup.find('img', id='edan-image')\n",
    "        if img_tag and img_tag.has_attr('src'):\n",
    "            img_src = img_tag['src']\n",
    "            # Parse the URL\n",
    "            parsed_url = urlparse(img_src)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            # Append '.jpg' to the 'id' parameter\n",
    "            if 'id' in query_params:\n",
    "                id_value = query_params['id'][0]\n",
    "                if not id_value.endswith('.jpg'):\n",
    "                    id_value += '.jpg'\n",
    "                query_params['id'] = [id_value]\n",
    "                # Reconstruct the query string\n",
    "                new_query = urlencode(query_params, doseq=True)\n",
    "                # Reconstruct the full URL\n",
    "                image_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}\"\n",
    "                item_dict[\"Image_URL\"] = image_url\n",
    "                logging.info(f\"    Image URL extracted from img tag: {image_url}\")\n",
    "            else:\n",
    "                logging.warning(f\"    No 'id' parameter in img src for {full_url}\")\n",
    "        else:\n",
    "            logging.warning(f\"    No valid Image URL found in {full_url}\")\n",
    "\n",
    "    return item_dict\n",
    "\n",
    "def scrape_smithsonian_collection(total_pages=3):\n",
    "    all_items = []\n",
    "    total_scraped = 0\n",
    "\n",
    "    # Initialize a session\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": random.choice(USER_AGENTS)\n",
    "    })\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = construct_url(page)\n",
    "        logging.info(f\"Scraping Page {page}: {url}\")\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.warning(f\"Request exception for page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        item_links = get_item_links(soup)\n",
    "        logging.info(f\"Found {len(item_links)} items on Page {page}\")\n",
    "\n",
    "        for idx, item_link in enumerate(item_links, 1):\n",
    "            logging.info(f\"  Scraping Item {idx}: {item_link}\")\n",
    "            item_info = extract_item_info(session, item_link)\n",
    "            if item_info:\n",
    "                all_items.append(item_info)\n",
    "                total_scraped += 1\n",
    "            else:\n",
    "                logging.warning(f\"    Failed to extract info for {item_link}\")\n",
    "\n",
    "            # Add a random pause between 1 to 3 seconds\n",
    "            sleep_time = random.uniform(1, 3)\n",
    "            logging.info(f\"    Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        logging.info(f\"Completed scraping Page {page}\\n\")\n",
    "\n",
    "    logging.info(\"Scraping completed.\")\n",
    "    # Convert list of dicts to DataFrame\n",
    "    df = pd.DataFrame(all_items)\n",
    "    return df\n",
    "\n",
    "# Load the NER pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "def extract_entities(df, entity_score_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Extracts named entities from the DataFrame's text columns and adds them as new columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing scraped data.\n",
    "    - entity_score_threshold (float): The minimum confidence score for entities to be considered.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): The updated DataFrame with new entity columns.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure the DataFrame index is unique\n",
    "    if not df.index.is_unique:\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Initialize a dictionary to hold the new columns\n",
    "    new_columns = {}\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Dictionary to hold entities for this row\n",
    "        row_entities = {}\n",
    "        \n",
    "        # Iterate over each column in the row\n",
    "        for column in df.columns:\n",
    "            # Skip columns that should not be processed for NER\n",
    "            if column in ['Title', 'Museum', 'Image_URL', 'cropped_objects']:\n",
    "                continue\n",
    "            \n",
    "            value = row[column]\n",
    "            \n",
    "            # Initialize list to hold text snippets to process\n",
    "            texts_to_process = []\n",
    "            \n",
    "            # Determine how to handle the value based on its type\n",
    "            if isinstance(value, list):\n",
    "                # If it's a list, check if all elements are null\n",
    "                if all(pd.isnull(v) for v in value):\n",
    "                    continue  # Skip if all elements are null\n",
    "                # Filter out non-string elements\n",
    "                texts_to_process = [str(v) for v in value if isinstance(v, str)]\n",
    "            elif isinstance(value, str):\n",
    "                if pd.isnull(value):\n",
    "                    continue  # Skip if the string is null\n",
    "                texts_to_process = [value]\n",
    "            else:\n",
    "                # For any other type (e.g., NaN, None), skip processing\n",
    "                continue\n",
    "            \n",
    "            # If there are no texts to process, skip to the next column\n",
    "            if not texts_to_process:\n",
    "                continue\n",
    "            \n",
    "            # Process each text snippet\n",
    "            for text in texts_to_process:\n",
    "                # Run the NER model on the text\n",
    "                try:\n",
    "                    results = token_classifier(text)\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"NER model failed on text '{text}': {e}\")\n",
    "                    continue  # Skip this text snippet if NER fails\n",
    "                \n",
    "                # Filter entities based on the score threshold\n",
    "                filtered_results = [entity for entity in results if entity['score'] >= entity_score_threshold]\n",
    "                \n",
    "                # Iterate over filtered entities and organize them by entity group\n",
    "                for entity in filtered_results:\n",
    "                    entity_group = entity['entity_group']\n",
    "                    word = entity['word'].replace('\\n', ' ').strip()  # Clean up the word\n",
    "                    \n",
    "                    # Create a new column name based on the original column and entity group\n",
    "                    col_name = f\"{column}.{entity_group}\"\n",
    "                    \n",
    "                    # Initialize the list for this entity group if not already present\n",
    "                    if col_name not in row_entities:\n",
    "                        row_entities[col_name] = []\n",
    "                    \n",
    "                    # Append the extracted word to the list\n",
    "                    row_entities[col_name].append(word)\n",
    "        \n",
    "        # Store the entities for this row\n",
    "        for col_name, words in row_entities.items():\n",
    "            if col_name not in new_columns:\n",
    "                new_columns[col_name] = [None] * len(df)\n",
    "            new_columns[col_name][idx] = words\n",
    "    \n",
    "    # After processing all rows, add the new columns to the DataFrame\n",
    "    for col_name, column_data in new_columns.items():\n",
    "        df[col_name] = column_data\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the processor and model for object detection\n",
    "processor = AutoImageProcessor.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "\n",
    "def analyze_and_crop_image(image_path, image_id, output_dir, category_data):\n",
    "    \"\"\"\n",
    "    Analyzes an image using the Fashionpedia model, saves cropped images, and returns a dictionary of objects data.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load category mappings\n",
    "    categories = category_data['categories']\n",
    "    category_id_to_name = {category['id']: category['name'] for category in categories}\n",
    "    category_id_to_supercategory = {category['id']: category['supercategory'] for category in categories}\n",
    "    label_mappings = model.config.id2label  # This maps label IDs to category names\n",
    "    name_to_supercategory = {category['name']: category['supercategory'] for category in categories}\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    # Ensure image is in RGB format\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=image_array, return_tensors=\"pt\")\n",
    "    # Run inference\n",
    "    outputs = model(**inputs)\n",
    "    # Post-process the outputs to get object detection results\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, threshold=0.5, target_sizes=[image.size[::-1]]\n",
    "    )\n",
    "    result = results[0]\n",
    "    \n",
    "    objects_data = {}\n",
    "    # Get image dimensions\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Loop over each detection\n",
    "    for idx, (score, label_id, box) in enumerate(zip(result['scores'], result['labels'], result['boxes'])):\n",
    "        score = score.item()\n",
    "        label_id = label_id.item()\n",
    "        box = box.tolist()\n",
    "        x_min, y_min, x_max, y_max = map(int, box)\n",
    "        area = (x_max - x_min) * (y_max - y_min)\n",
    "        name = label_mappings.get(label_id, 'Unknown')\n",
    "        supercategory = name_to_supercategory.get(name, 'Unknown')\n",
    "        category_id = next((id for id, n in category_id_to_name.items() if n == name), label_id)\n",
    "        \n",
    "        # Save the cropped image\n",
    "        # Clip coordinates to image bounds\n",
    "        x_min = max(0, min(width, x_min))\n",
    "        y_min = max(0, min(height, y_min))\n",
    "        x_max = max(0, min(width, x_max))\n",
    "        y_max = max(0, min(height, y_max))\n",
    "        \n",
    "        # Check for valid crop\n",
    "        if x_max > x_min and y_max > y_min:\n",
    "            # Crop the image\n",
    "            cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "            # Ensure cropped image is in RGB mode\n",
    "            if cropped_image.mode != 'RGB':\n",
    "                cropped_image = cropped_image.convert('RGB')\n",
    "            # Create a unique filename\n",
    "            cropped_image_name = f\"{os.path.splitext(os.path.basename(image_path))[0]}_{name.replace(' ', '_')}_{idx}.jpg\"\n",
    "            cropped_image_path = os.path.join(output_dir, cropped_image_name)\n",
    "            try:\n",
    "                # Save the cropped image\n",
    "                cropped_image.save(cropped_image_path, format='JPEG')\n",
    "                print(f\"Saved cropped image: {cropped_image_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save {cropped_image_path}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Skipping invalid crop for detection {idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Collect the data\n",
    "        object_key = f\"Object_{idx+1}\"\n",
    "        objects_data[object_key] = {\n",
    "            'Name': name,\n",
    "            'Supercategory': supercategory,\n",
    "            'Bounding Box': box,\n",
    "            'Area': area,\n",
    "            'cropped_image_path': cropped_image_path\n",
    "        }\n",
    "    return objects_data\n",
    "\n",
    "def process_images(df, output_dir='cropped_images', category_json_path='../category_attributes_descriptions.json'):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load category mappings\n",
    "    with open(category_json_path, 'r') as f:\n",
    "        category_data = json.load(f)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        image_url = row['Image_URL']\n",
    "        if pd.isnull(image_url):\n",
    "            continue\n",
    "        print(f\"Processing image for row {idx}: {image_url}\")\n",
    "        # Download the image\n",
    "        image_name = f\"image_{idx}.jpg\"\n",
    "        image_path = os.path.join(output_dir, image_name)\n",
    "        try:\n",
    "            response = requests.get(image_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            with open(image_path, 'wb') as out_file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        out_file.write(chunk)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download image {image_url}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Process the image\n",
    "        try:\n",
    "            objects_data = analyze_and_crop_image(image_path, idx, output_dir, category_data)\n",
    "            df.at[idx, 'cropped_objects'] = objects_data\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process image {image_path}: {e}\")\n",
    "            continue\n",
    "    return df\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape the data\n",
    "    scraped_df = scrape_smithsonian_collection(total_pages=1)\n",
    "    print(f\"Scraping Completed. Total items scraped: {len(scraped_df)}\")\n",
    "    # Save the scraped data to a CSV file\n",
    "    scraped_df.to_csv('scraped_data.csv', index=False)\n",
    "    \n",
    "    # Extract entities\n",
    "    print(\"Extracting entities...\")\n",
    "    scraped_df = extract_entities(scraped_df, entity_score_threshold=0.8)\n",
    "    # Save the data with entities to a CSV file\n",
    "    scraped_df.to_csv('scraped_data_with_entities.csv', index=False)\n",
    "    \n",
    "    # Process images\n",
    "    print(\"Processing images...\")\n",
    "    scraped_df = process_images(scraped_df, output_dir='cropped_images', category_json_path='/Users/joshstrupp/Documents/Working/Educational/MSDV/ms1-final/category_attributes_descriptions.json')\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    scraped_df.to_csv('final_data.csv', index=False)\n",
    "    # Optionally, save the DataFrame to a pickle file\n",
    "    scraped_df.to_pickle('final_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
