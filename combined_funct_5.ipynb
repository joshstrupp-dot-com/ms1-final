{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "Loaded 4220 existing records from 'final_data.pkl'.\n",
      "Loaded 4220 existing objects from 'final_data.pkl'.\n",
      "No checkpoints found in 'checkpoints_2'. Starting from page 1.\n",
      "Scraping Page 1\n",
      "Retrieving item 1: https://www.si.edu/object/fashion-illustration:nmah_375563\n",
      "/var/folders/pc/j_zmpc9s0g9frv2jzfv1l51c0000gn/T/ipykernel_91962/2722636005.py:111: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  screen_image_link = soup.find('a', text='Screen Image')\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-88-952.jpg\n",
      "Retrieving item 2: https://www.si.edu/object/fashions:nmah_351081\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27197.jpg\n",
      "Retrieving item 3: https://www.si.edu/object/fashion-plate:nmah_352765\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-89-251.jpg\n",
      "Retrieving item 4: https://www.si.edu/object/fashion-plate:nmah_352737\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27138.jpg\n",
      "Retrieving item 5: https://www.si.edu/object/fashion-plate:nmah_352733\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27132.jpg\n",
      "Retrieving item 6: https://www.si.edu/object/fashion-plate:nmah_352731\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27131.jpg\n",
      "Retrieving item 7: https://www.si.edu/object/fashion-plate:nmah_352759\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27154.jpg\n",
      "Retrieving item 8: https://www.si.edu/object/fashion-plate:nmah_352753\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27146.jpg\n",
      "Retrieving item 9: https://www.si.edu/object/fashion-plate:nmah_371552\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27446.jpg\n",
      "Retrieving item 10: https://www.si.edu/object/fashion-plate:nmah_352745\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27139.jpg\n",
      "Retrieving item 11: https://www.si.edu/object/fashion-plate:nmah_352754\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27147.jpg\n",
      "Retrieving item 12: https://www.si.edu/object/fashion-plate:nmah_352708\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27121.jpg\n",
      "Retrieving item 13: https://www.si.edu/object/fashion-plate:nmah_352736\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27137.jpg\n",
      "Retrieving item 14: https://www.si.edu/object/fashion-plate:nmah_353210\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27305.jpg\n",
      "Retrieving item 15: https://www.si.edu/object/fashion-plate:nmah_375088\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-87-6691.jpg\n",
      "Retrieving item 16: https://www.si.edu/object/fashion-plate:nmah_373482\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-12032.jpg\n",
      "Retrieving item 17: https://www.si.edu/object/fashion-plate:nmah_373490\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-12028.jpg\n",
      "Retrieving item 18: https://www.si.edu/object/fashion-plate:nmah_375081\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-87-6694.jpg\n",
      "Retrieving item 19: https://www.si.edu/object/fashion-plate:nmah_375074\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-86-11660.jpg\n",
      "Retrieving item 20: https://www.si.edu/object/fashion-plate:nmah_375104\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-86-7378.jpg\n",
      "Retrieving item 21: https://www.si.edu/object/fashion-plate:nmah_375072\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-86-11654.jpg\n",
      "Retrieving item 22: https://www.si.edu/object/fashion-plate:nmah_373503\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-15354.jpg\n",
      "Retrieving item 23: https://www.si.edu/object/fashion-plate:nmah_375083\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-87-6700.jpg\n",
      "Retrieving item 24: https://www.si.edu/object/fashion-plate:nmah_375103\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-86-7377.jpg\n",
      "Retrieving item 25: https://www.si.edu/object/fashion-plate:nmah_373506\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-15344.jpg\n",
      "Retrieving item 26: https://www.si.edu/object/fashion-plate:nmah_373500\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-15357.jpg\n",
      "Retrieving item 27: https://www.si.edu/object/fashion-plate:nmah_375097\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-87-6690.jpg\n",
      "Retrieving item 28: https://www.si.edu/object/fashion-plate:nmah_373468\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-9913.jpg\n",
      "Retrieving item 29: https://www.si.edu/object/fashion-plate:nmah_375096\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-89-260.jpg\n",
      "Retrieving item 30: https://www.si.edu/object/fashion-plate:nmah_375085\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-86-11662.jpg\n",
      "Retrieving item 31: https://www.si.edu/object/fashion-plate:nmah_375082\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-87-6695.jpg\n",
      "Retrieving item 32: https://www.si.edu/object/fashion-plate:nmah_373498\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-15356.jpg\n",
      "Retrieving item 33: https://www.si.edu/object/fashion-plate:nmah_375067\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-86-11648.jpg\n",
      "Retrieving item 34: https://www.si.edu/object/fashion-plate:nmah_373497\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-84-15358.jpg\n",
      "Retrieving item 35: https://www.si.edu/object/fashion-plate:nmah_351540\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26780.jpg\n",
      "Retrieving item 36: https://www.si.edu/object/fashion-plate:nmah_351416\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26723.jpg\n",
      "Retrieving item 37: https://www.si.edu/object/fashion-plate:nmah_351350\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26700.jpg\n",
      "Retrieving item 38: https://www.si.edu/object/fashion-plate:nmah_351502\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26775.jpg\n",
      "Retrieving item 39: https://www.si.edu/object/fashion-plate:nmah_351661\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26823.jpg\n",
      "Retrieving item 40: https://www.si.edu/object/fashion-plate:nmah_351386\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26718.jpg\n",
      "Retrieving item 41: https://www.si.edu/object/fashion-plate:nmah_351331\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26809.jpg\n",
      "Retrieving item 42: https://www.si.edu/object/fashion-plate:nmah_351480\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26767.jpg\n",
      "Retrieving item 43: https://www.si.edu/object/fashion-plate:nmah_351527\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26848.jpg\n",
      "Retrieving item 44: https://www.si.edu/object/fashion-plate:nmah_351669\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26842.jpg\n",
      "Retrieving item 45: https://www.si.edu/object/fashion-plate:nmah_351404\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26712.jpg\n",
      "Retrieving item 46: https://www.si.edu/object/fashion-plate:nmah_351328\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26686.jpg\n",
      "Retrieving item 47: https://www.si.edu/object/godeys-fashions:nmah_351174\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26887.jpg\n",
      "Retrieving item 48: https://www.si.edu/object/fashion-plate:nmah_351413\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26725.jpg\n",
      "Retrieving item 49: https://www.si.edu/object/fashion-plate:nmah_351396\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26717.jpg\n",
      "Retrieving item 50: https://www.si.edu/object/fashion-plate:nmah_352712\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27062.jpg\n",
      "Retrieving item 51: https://www.si.edu/object/fashion-plate:nmah_351418\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26721.jpg\n",
      "Retrieving item 52: https://www.si.edu/object/fashion-plate:nmah_352713\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27063.jpg\n",
      "Retrieving item 53: https://www.si.edu/object/fashion-plate:nmah_351420\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26724.jpg\n",
      "Retrieving item 54: https://www.si.edu/object/fashion-plate:nmah_351654\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26824.jpg\n",
      "Saved checkpoint for page 1\n",
      "Scraping Page 2\n",
      "Retrieving item 1: https://www.si.edu/object/fashion-plate:nmah_351687\n",
      "/var/folders/pc/j_zmpc9s0g9frv2jzfv1l51c0000gn/T/ipykernel_91962/2722636005.py:111: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  screen_image_link = soup.find('a', text='Screen Image')\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26831.jpg\n",
      "Retrieving item 2: https://www.si.edu/object/fashion-plate:nmah_351353\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26706.jpg\n",
      "Retrieving item 3: https://www.si.edu/object/fashion-plate:nmah_351389\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26716.jpg\n",
      "Retrieving item 4: https://www.si.edu/object/fashion-plate:nmah_351402\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26710.jpg\n",
      "Retrieving item 5: https://www.si.edu/object/fashion-plate:nmah_351541\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26783.jpg\n",
      "Retrieving item 6: https://www.si.edu/object/fashion-plate:nmah_351478\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26770.jpg\n",
      "Retrieving item 7: https://www.si.edu/object/fashion-plate:nmah_351333\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26684.jpg\n",
      "Retrieving item 8: https://www.si.edu/object/fashion-plate:nmah_351658\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26829.jpg\n",
      "Retrieving item 9: https://www.si.edu/object/fashion-plate:nmah_351352\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26709.jpg\n",
      "Retrieving item 10: https://www.si.edu/object/fashion-plate:nmah_352726\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27034.jpg\n",
      "Retrieving item 11: https://www.si.edu/object/fashion-plate:nmah_353208\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26947.jpg\n",
      "Retrieving item 12: https://www.si.edu/object/fashion-plate:nmah_351684\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26833.jpg\n",
      "Retrieving item 13: https://www.si.edu/object/fashion-plate:nmah_351308\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26685.jpg\n",
      "Retrieving item 14: https://www.si.edu/object/fashion-plate:nmah_351489\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26777.jpg\n",
      "Retrieving item 15: https://www.si.edu/object/fashion-plate:nmah_351329\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26808.jpg\n",
      "Retrieving item 16: https://www.si.edu/object/fashion-plate:nmah_351672\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26843.jpg\n",
      "Retrieving item 17: https://www.si.edu/object/fashion-plate:nmah_351375\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26699.jpg\n",
      "Retrieving item 18: https://www.si.edu/object/fashion-plate:nmah_351370\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26702.jpg\n",
      "Retrieving item 19: https://www.si.edu/object/fashion-plate:nmah_351374\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26697.jpg\n",
      "Retrieving item 20: https://www.si.edu/object/fashion-plate:nmah_351603\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26811.jpg\n",
      "Retrieving item 21: https://www.si.edu/object/fashion-plate:nmah_351707\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26849.jpg\n",
      "Retrieving item 22: https://www.si.edu/object/fashion-plate:nmah_351394\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26714.jpg\n",
      "Retrieving item 23: https://www.si.edu/object/fashion-plate:nmah_351558\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26793.jpg\n",
      "Retrieving item 24: https://www.si.edu/object/fashion-plate:nmah_351499\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26776.jpg\n",
      "Retrieving item 25: https://www.si.edu/object/fashion-plate:nmah_351254\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26748.jpg\n",
      "Retrieving item 26: https://www.si.edu/object/fashion-plate:nmah_351316\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26689.jpg\n",
      "Retrieving item 27: https://www.si.edu/object/fashion-plate:nmah_351621\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26815.jpg\n",
      "Retrieving item 28: https://www.si.edu/object/fashion-plate:nmah_351531\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26773.jpg\n",
      "Retrieving item 29: https://www.si.edu/object/fashion-plate:nmah_351403\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26713.jpg\n",
      "Retrieving item 30: https://www.si.edu/object/fashion-plate:nmah_351497\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27539.jpg\n",
      "Retrieving item 31: https://www.si.edu/object/fashion-plate:nmah_351629\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26813.jpg\n",
      "Retrieving item 32: https://www.si.edu/object/fashion-plate:nmah_352722\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27035.jpg\n",
      "Retrieving item 33: https://www.si.edu/object/fashion-plate:nmah_351504\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26778.jpg\n",
      "Retrieving item 34: https://www.si.edu/object/fashion-plate:nmah_351710\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26845.jpg\n",
      "Retrieving item 35: https://www.si.edu/object/fashion-plate:nmah_351522\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26782.jpg\n",
      "Retrieving item 36: https://www.si.edu/object/fashion-plate:nmah_351345\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26701.jpg\n",
      "Retrieving item 37: https://www.si.edu/object/fashion-plate:nmah_351318\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26690.jpg\n",
      "Retrieving item 38: https://www.si.edu/object/fashion-plate:nmah_351650\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26826.jpg\n",
      "Retrieving item 39: https://www.si.edu/object/fashion-plate:nmah_351319\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26691.jpg\n",
      "Retrieving item 40: https://www.si.edu/object/fashion-plate:nmah_351625\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26817.jpg\n",
      "Retrieving item 41: https://www.si.edu/object/fashion-plate:nmah_351542\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26781.jpg\n",
      "Retrieving item 42: https://www.si.edu/object/fashion-plate:nmah_351330\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26687.jpg\n",
      "Retrieving item 43: https://www.si.edu/object/fashion-plate:nmah_351409\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26720.jpg\n",
      "Retrieving item 44: https://www.si.edu/object/fashion-plate:nmah_351392\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26715.jpg\n",
      "Retrieving item 45: https://www.si.edu/object/fashion-plate:nmah_689854\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26844.jpg\n",
      "Retrieving item 46: https://www.si.edu/object/fashion-plate:nmah_351087\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26730.jpg\n",
      "Retrieving item 47: https://www.si.edu/object/fashion-plate:nmah_351397\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26719.jpg\n",
      "Retrieving item 48: https://www.si.edu/object/fashion-plate:nmah_351317\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26688.jpg\n",
      "Retrieving item 49: https://www.si.edu/object/fashion-plate:nmah_351449\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26765.jpg\n",
      "Retrieving item 50: https://www.si.edu/object/fashion-plate:nmah_352707\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27074.jpg\n",
      "Retrieving item 51: https://www.si.edu/object/fashion-plate:nmah_351327\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-27538.jpg\n",
      "Retrieving item 52: https://www.si.edu/object/fashion-plate:nmah_351662\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26835.jpg\n",
      "Retrieving item 53: https://www.si.edu/object/fashion-plate:nmah_351339\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26703.jpg\n",
      "Retrieving item 54: https://www.si.edu/object/fashion-plate:nmah_351381\n",
      "    Image URL extracted from img tag: https://ids.si.edu/ids/deliveryService?max_w=800&id=NMAH-99-26705.jpg\n",
      "Saved checkpoint for page 2\n",
      "Scraping Completed. Total objects scraped: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Completed. Total items scraped: 50\n",
      "Final data saved with NER information. Total records: 4270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Base URL components\n",
    "# BASE_URL = \"https://www.si.edu/search/collection-images?edan_q=&edan_fq%5B0%5D=topic%3A%22Costume%22\"\n",
    "# BASE_URL = \"\n",
    "# https://www.si.edu/search/collection-images?edan_q=&edan_fq%5B%5D=media_usage:%22CC0%22&edan_fq%5B%5D=topic:%22Costume%22+OR+topic%3A%22Dress+accessories%22+OR+topic%3A%22Hats%22+OR+topic%3A%22Headgear%22+OR+topic%3A%22Neckties%22+OR+topic%3A%22Neckwear%22\"\n",
    "\n",
    "BASE_URL = 'https://www.si.edu/search/collection-images?edan_q=fashion'\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)\",\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "# Function to construct URL with page number\n",
    "def construct_url(page_number):\n",
    "    if page_number > 1:\n",
    "        # Insert page parameter after collection-images? for subsequent pages\n",
    "        page_param = f\"page={page_number - 1}&\"\n",
    "        url_parts = BASE_URL.split(\"?\")\n",
    "        return f\"{url_parts[0]}?{page_param}{url_parts[1]}\"\n",
    "    return BASE_URL\n",
    "\n",
    "# Function to extract item links from a page\n",
    "def get_item_links(soup):\n",
    "    item_links = []\n",
    "    for li in soup.find_all(\"li\", attrs={\"ogmt-id\": True}):\n",
    "        a_tag = li.find(\"a\", class_=\"inner\")\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            item_links.append(a_tag['href'])\n",
    "    return item_links\n",
    "\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "\n",
    "def extract_item_info(session, item_url, item_number=None):\n",
    "    full_url = f\"https://www.si.edu{item_url}\"\n",
    "\n",
    "    # Extract item ID from item_url\n",
    "    item_id = item_url.split('/')[-1]\n",
    "    item_dict = {'Item_ID': item_id}  # Initialize item_dict with Item_ID\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = session.get(full_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.warning(f\"Request exception for item {item_number}: {full_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    content = response.content\n",
    "    logging.info(f\"Retrieving item {item_number}: {full_url}\")\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Extract the h1 tag\n",
    "    h1_tag = soup.find(\"h1\")\n",
    "    if not h1_tag:\n",
    "        logging.warning(f\"No h1 tag found in {full_url}\")\n",
    "        return None\n",
    "    h1_text = h1_tag.get_text(strip=True)\n",
    "\n",
    "    # Initialize the item dictionary\n",
    "    item_dict = {'Title': h1_text}\n",
    "\n",
    "    # Extract the \"Museum\" information from the next <a> tag after <h1>\n",
    "    next_a_tag = h1_tag.find_next(\"a\")\n",
    "    if next_a_tag and next_a_tag.get_text(strip=True):\n",
    "        museum_name = next_a_tag.get_text(strip=True)\n",
    "        item_dict[\"Museum\"] = museum_name\n",
    "    else:\n",
    "        logging.warning(f\"No Museum information found in {full_url}\")\n",
    "\n",
    "    # Iterate through all <dl> tags directly\n",
    "    for dl in soup.find_all(\"dl\"):\n",
    "        current_key = None\n",
    "        values = []\n",
    "        for child in dl.children:\n",
    "            if child.name == \"dt\":\n",
    "                if current_key and values:\n",
    "                    # Assign the collected values to the previous key\n",
    "                    item_dict[current_key] = values\n",
    "                # Start a new key\n",
    "                current_key = child.get_text(strip=True)\n",
    "                values = []\n",
    "            elif child.name == \"dd\":\n",
    "                dd_text = child.get_text(strip=True)\n",
    "                values.append(dd_text)\n",
    "        # After the loop, assign the last collected values\n",
    "        if current_key and values:\n",
    "            item_dict[current_key] = values\n",
    "\n",
    "    # Try to extract the 'Screen Image' link\n",
    "    screen_image_link = soup.find('a', text='Screen Image')\n",
    "    if screen_image_link and screen_image_link.has_attr('href'):\n",
    "        image_url = screen_image_link['href']\n",
    "        if not image_url.startswith('http'):\n",
    "            image_url = f\"https://ids.si.edu{image_url}\"\n",
    "        item_dict[\"Image_URL\"] = image_url\n",
    "        logging.info(f\"    Screen Image URL extracted: {image_url}\")\n",
    "    else:\n",
    "        # Fallback method: extract from <img id='edan-image'> tag\n",
    "        img_tag = soup.find('img', id='edan-image')\n",
    "        if img_tag and img_tag.has_attr('src'):\n",
    "            img_src = img_tag['src']\n",
    "            # Parse the URL\n",
    "            parsed_url = urlparse(img_src)\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            # Append '.jpg' to the 'id' parameter\n",
    "            if 'id' in query_params:\n",
    "                id_value = query_params['id'][0]\n",
    "                if not id_value.endswith('.jpg'):\n",
    "                    id_value += '.jpg'\n",
    "                query_params['id'] = [id_value]\n",
    "                # Reconstruct the query string\n",
    "                new_query = urlencode(query_params, doseq=True)\n",
    "                # Reconstruct the full URL\n",
    "                image_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}\"\n",
    "                item_dict[\"Image_URL\"] = image_url\n",
    "                logging.info(f\"    Image URL extracted from img tag: {image_url}\")\n",
    "            else:\n",
    "                logging.warning(f\"    No 'id' parameter in img src for {full_url}\")\n",
    "        else:\n",
    "            logging.warning(f\"    No valid Image URL found in {full_url}\")\n",
    "\n",
    "    return item_dict\n",
    "\n",
    "def scrape_smithsonian_collection(\n",
    "    total_pages=3,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    output_dir='archive/cropped_images',\n",
    "    resume=False\n",
    "):\n",
    "    # Ensure necessary directories exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('final_data.pkl'):\n",
    "        existing_df = pd.read_pickle('final_data.pkl')\n",
    "        all_objects = existing_df.to_dict('records')\n",
    "        total_items = len(all_objects)\n",
    "        logging.info(f\"Loaded {total_items} existing objects from 'final_data.pkl'.\")\n",
    "    else:\n",
    "        all_objects = []\n",
    "        total_items = 0\n",
    "    \n",
    "    # Initialize list to collect all objects\n",
    "    all_objects = []\n",
    "    \n",
    "    # Handle checkpointing for resuming\n",
    "    if resume:\n",
    "        checkpoints = sorted([\n",
    "            f for f in os.listdir(checkpoint_dir) \n",
    "            if f.endswith('.pkl') and f.startswith('checkpoint_')\n",
    "        ])\n",
    "        if checkpoints:\n",
    "            last_checkpoint = checkpoints[-1]\n",
    "            start_page = int(last_checkpoint.split('_')[1].split('.')[0]) + 1\n",
    "            # Load last checkpoint data\n",
    "            all_objects = pd.read_pickle(os.path.join(checkpoint_dir, last_checkpoint)).to_dict('records')\n",
    "            logging.info(f\"Resuming from page {start_page} with {len(all_objects)} objects loaded from checkpoint.\")\n",
    "        else:\n",
    "            start_page = 1\n",
    "            logging.warning(f\"No checkpoints found in '{checkpoint_dir}'. Starting from page {start_page}.\")\n",
    "    else:\n",
    "        start_page = 1\n",
    "        logging.info(\"Starting scraping from page 1 without resuming.\")\n",
    "    \n",
    "    # Initialize the requests session\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": random.choice(USER_AGENTS)})\n",
    "    \n",
    "    # Initialize total_items to keep track of the cumulative item index\n",
    "    total_items = len(all_objects) if resume else 0\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page in range(start_page, total_pages + 1):\n",
    "        logging.info(f\"Scraping Page {page}\")\n",
    "        \n",
    "        try:\n",
    "            # Construct and send the GET request\n",
    "            url = construct_url(page)\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the page content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            item_links = get_item_links(soup)\n",
    "            \n",
    "            page_items = []\n",
    "            for idx, item_link in enumerate(item_links, 1):\n",
    "                item_info = extract_item_info(session, item_link, item_number=idx)\n",
    "                if item_info:\n",
    "                    page_items.append(item_info)\n",
    "                time.sleep(random.uniform(1, 3))  # Respectful scraping\n",
    "            \n",
    "            # Convert the items to a DataFrame\n",
    "            page_df = pd.DataFrame(page_items)\n",
    "            \n",
    "            # Set unique index for page_df\n",
    "            page_df.index = range(total_items, total_items + len(page_df))\n",
    "            \n",
    "            # Process images and extract objects\n",
    "            objects_df = process_images(page_df, output_dir)\n",
    "            all_objects.extend(objects_df.to_dict('records'))\n",
    "            \n",
    "            # Update the total_items counter\n",
    "            total_items += len(page_df)\n",
    "            \n",
    "            # Save checkpoint after processing the page\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_{page}.pkl')\n",
    "            pd.DataFrame(all_objects).to_pickle(checkpoint_path)\n",
    "            logging.info(f\"Saved checkpoint for page {page}\")\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 404:\n",
    "                logging.warning(f\"Page {page} does not exist (404). Stopping scraper.\")\n",
    "                break  # Exit the loop if the page doesn't exist\n",
    "            else:\n",
    "                logging.error(f\"HTTP error occurred on page {page}: {http_err}\")\n",
    "                continue  # Skip to the next page\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error on page {page}: {e}\")\n",
    "            continue  # Skip to the next page\n",
    "    \n",
    "    # Create the final DataFrame from all collected objects\n",
    "    final_df = pd.DataFrame(all_objects)\n",
    "    \n",
    "    # Save the final DataFrame\n",
    "    final_df.to_csv('final_data.csv', index=False)\n",
    "    final_df.to_pickle('final_data.pkl')\n",
    "    \n",
    "    logging.info(f\"Scraping Completed. Total objects scraped: {len(final_df)}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "def extract_entities(df, entity_score_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Extracts named entities from the DataFrame's text columns and adds them as new columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing scraped data.\n",
    "    - entity_score_threshold (float): The minimum confidence score for entities to be considered.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): The updated DataFrame with new entity columns.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure the DataFrame index is unique\n",
    "    if not df.index.is_unique:\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Initialize a dictionary to hold the new columns\n",
    "    new_columns = {}\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Dictionary to hold entities for this row\n",
    "        row_entities = {}\n",
    "        \n",
    "        # Iterate over each column in the row\n",
    "        for column in df.columns:\n",
    "            # Skip columns that should not be processed for NER\n",
    "            if column in ['Title', 'Museum', 'Image_URL', 'cropped_objects']:\n",
    "                continue\n",
    "            \n",
    "            value = row[column]\n",
    "            \n",
    "            # Initialize list to hold text snippets to process\n",
    "            texts_to_process = []\n",
    "            \n",
    "            # Determine how to handle the value based on its type\n",
    "            if isinstance(value, list):\n",
    "                # If it's a list, check if all elements are null\n",
    "                if all(pd.isnull(v) for v in value):\n",
    "                    continue  # Skip if all elements are null\n",
    "                # Filter out non-string elements\n",
    "                texts_to_process = [str(v) for v in value if isinstance(v, str)]\n",
    "            elif isinstance(value, str):\n",
    "                if pd.isnull(value):\n",
    "                    continue  # Skip if the string is null\n",
    "                texts_to_process = [value]\n",
    "            else:\n",
    "                # For any other type (e.g., NaN, None), skip processing\n",
    "                continue\n",
    "            \n",
    "            # If there are no texts to process, skip to the next column\n",
    "            if not texts_to_process:\n",
    "                continue\n",
    "            \n",
    "            # Process each text snippet\n",
    "            for text in texts_to_process:\n",
    "                # Run the NER model on the text\n",
    "                try:\n",
    "                    results = token_classifier(text)\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"NER model failed on text '{text}': {e}\")\n",
    "                    continue  # Skip this text snippet if NER fails\n",
    "                \n",
    "                # Filter entities based on the score threshold\n",
    "                filtered_results = [entity for entity in results if entity['score'] >= entity_score_threshold]\n",
    "                \n",
    "                # Iterate over filtered entities and organize them by entity group\n",
    "                for entity in filtered_results:\n",
    "                    entity_group = entity['entity_group']\n",
    "                    word = entity['word'].replace('\\n', ' ').strip()  # Clean up the word\n",
    "                    \n",
    "                    # Create a new column name based on the original column and entity group\n",
    "                    col_name = f\"{column}.{entity_group}\"\n",
    "                    \n",
    "                    # Initialize the list for this entity group if not already present\n",
    "                    if col_name not in row_entities:\n",
    "                        row_entities[col_name] = []\n",
    "                    \n",
    "                    # Append the extracted word to the list\n",
    "                    row_entities[col_name].append(word)\n",
    "        \n",
    "        # Store the entities for this row\n",
    "        for col_name, words in row_entities.items():\n",
    "            if col_name not in new_columns:\n",
    "                new_columns[col_name] = [None] * len(df)\n",
    "            new_columns[col_name][idx] = words\n",
    "    \n",
    "    # After processing all rows, add the new columns to the DataFrame\n",
    "    for col_name, column_data in new_columns.items():\n",
    "        df[col_name] = column_data\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the processor and model for object detection\n",
    "processor = AutoImageProcessor.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "\n",
    "# Get the label mapping\n",
    "id2label = model.config.id2label  # mapping from label IDs to label names\n",
    "\n",
    "def analyze_and_crop_image(image_path, image_index, output_dir, item_id, confidence_threshold=0.75):\n",
    "    \"\"\"\n",
    "    Analyzes the image, crops objects, and returns object information.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the downloaded image.\n",
    "        image_index (int): Index of the image.\n",
    "        output_dir (str): Directory to save cropped images.\n",
    "        item_id (str): ID of the item being processed.\n",
    "        confidence_threshold (float): Confidence threshold for detections.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of objects with their attributes.\n",
    "    \"\"\"\n",
    "    objects = {}\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Preprocess the image\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Perform inference\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Convert outputs to COCO API\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = processor.post_process_object_detection(outputs, threshold=confidence_threshold, target_sizes=target_sizes)[0]\n",
    "        \n",
    "        # Iterate over detections\n",
    "        for idx, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
    "            # Convert label to category name\n",
    "            category_id = label.item()\n",
    "            category_name = id2label[category_id]\n",
    "            \n",
    "            # Convert box coordinates to integers\n",
    "            xmin, ymin, xmax, ymax = map(int, box.tolist())\n",
    "            bbox = [xmin, ymin, xmax, ymax]\n",
    "            area = (xmax - xmin) * (ymax - ymin)\n",
    "            \n",
    "            # Create a unique identifier for the object\n",
    "            obj_index = idx  # You can adjust this as needed\n",
    "            obj_id = f\"{category_name}_{obj_index}\"\n",
    "            \n",
    "            # Crop the image\n",
    "            cropped_img = image.crop((xmin, ymin, xmax, ymax))\n",
    "            cropped_image_name = f\"image_{item_id}_{obj_id}.jpg\"\n",
    "            cropped_image_path = os.path.join(output_dir, cropped_image_name)\n",
    "            cropped_img.save(cropped_image_path)\n",
    "            \n",
    "            # Collect object info\n",
    "            obj = {\n",
    "                'Name': category_name,\n",
    "                'Bounding Box': bbox,\n",
    "                'Area': area,\n",
    "                'Confidence': score.item(),\n",
    "                'cropped_image_path': cropped_image_path\n",
    "            }\n",
    "            objects[obj_id] = obj\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {e}\")\n",
    "        \n",
    "    return objects\n",
    "def process_images(df, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    object_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        image_url = row['Image_URL']\n",
    "        item_id = row.get('Item_ID', f\"item_{idx}\")\n",
    "        image_name = f\"image_{item_id}.jpg\"\n",
    "        image_path = os.path.join(output_dir, image_name)\n",
    "        try:\n",
    "            response = requests.get(image_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            with open(image_path, 'wb') as out_file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        out_file.write(chunk)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to download image {image_url}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Process the image\n",
    "        try:\n",
    "            objects_data = analyze_and_crop_image(\n",
    "                image_path=image_path,\n",
    "                image_index=idx,\n",
    "                output_dir=output_dir,\n",
    "                item_id=item_id,  # Pass item_id here\n",
    "                confidence_threshold=0.9\n",
    "            )\n",
    "\n",
    "            for object_id, object_info in objects_data.items():\n",
    "                # Add image index or any other relevant info\n",
    "                object_info['Image_Index'] = idx\n",
    "                # Include the object ID\n",
    "                object_info['Object_ID'] = object_id\n",
    "                \n",
    "                # **Integrate Item-Level Data:**\n",
    "                # Add all columns from the item (row) to the object_info\n",
    "                for key, value in row.items():\n",
    "                    object_info[key] = value\n",
    "                \n",
    "                object_rows.append(object_info)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process image {image_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create a DataFrame where each row is an object with integrated item-level data\n",
    "    objects_df = pd.DataFrame(object_rows)\n",
    "    # Set 'Object_ID' as the index if desired\n",
    "    objects_df.set_index('Object_ID', inplace=True)\n",
    "    return objects_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import logging\n",
    "\n",
    "    \n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Define scraping parameters\n",
    "    TOTAL_PAGES = 2  # Adjust as needed\n",
    "    CHECKPOINT_DIR = 'checkpoints_2'\n",
    "    OUTPUT_DIR = 'archive/cropped_images_2'\n",
    "    RESUME = True  # Set to True to resume from last checkpoint\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('final_data.pkl'):\n",
    "        existing_df = pd.read_pickle('final_data.pkl')\n",
    "        logging.info(f\"Loaded {len(existing_df)} existing records from 'final_data.pkl'.\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        logging.info(\"No existing data found. Starting fresh.\")\n",
    "\n",
    "    # Scrape the data with checkpointing\n",
    "    scraped_df = scrape_smithsonian_collection(\n",
    "        total_pages=TOTAL_PAGES,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        resume=RESUME\n",
    "    )\n",
    "\n",
    "    print(f\"Scraping Completed. Total items scraped: {len(scraped_df)}\")\n",
    "    \n",
    "    # **Extract Named Entities**\n",
    "    enriched_df = extract_entities(scraped_df, entity_score_threshold=0.8)\n",
    "    \n",
    "    # Combine with existing data\n",
    "    combined_df = pd.concat([existing_df, enriched_df], ignore_index=True)\n",
    "\n",
    "    # **Save Final Results**\n",
    "    combined_df.to_csv('final_data.csv', index=False)\n",
    "    combined_df.to_pickle('final_data.pkl')\n",
    "    \n",
    "    print(f\"Final data saved with NER information. Total records: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/j_zmpc9s0g9frv2jzfv1l51c0000gn/T/ipykernel_91962/208239470.py:4: DtypeWarning: Columns (8,16,17,18,24,25,26,27,28,41,42,43,44,45,46,47,48,52,54,55,56,62,65,70,71,73,75,76,77,79,81,82,83,90,96,100,104,105,106,113,115,117,120,121,122,123,124,125,127,136,137,139,141,143,146,147,150,151,154,156,159,160,168,169,173,174,176,177) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/Users/joshstrupp/Documents/Working/Educational/MSDV/ms1-final/final_data.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the final data\n",
    "df = pd.read_csv('/Users/joshstrupp/Documents/Working/Educational/MSDV/ms1-final/final_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
